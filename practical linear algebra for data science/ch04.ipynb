{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NOTE: these lines define global figure properties used for publication.\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg') # print figures in svg format\n",
    "plt.rcParams.update({'font.size':14}) # set global font size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "\n",
    "# correlated random var\n",
    "x = np.linspace(0,10,N) + np.random.rand(N)\n",
    "y = x + np.random.randn(N)\n",
    "\n",
    "# setup figure\n",
    "_, axs = plt.subplots(2, 2, figsize=(6,6))\n",
    "\n",
    "def common_axs_setting(axs):\n",
    "    axs.set_xlabel(\"Variable x\")\n",
    "    axs.set_ylabel(\"Variable y\")\n",
    "    axs.set_xticks([])\n",
    "    axs.set_yticks([])\n",
    "    axs.axis(\"square\")\n",
    "\n",
    "axs[0,0].plot(x, y, \"ko\")\n",
    "axs[0,0].set_title(\"Positive correlation\", fontweight=\"bold\")\n",
    "common_axs_setting(axs[0,0])\n",
    "\n",
    "axs[0,1].plot(x, -y, \"ko\")\n",
    "axs[0,1].set_title(\"negative correlation\", fontweight=\"bold\")\n",
    "common_axs_setting(axs[0,1])\n",
    "\n",
    "axs[1,0].plot(np.random.randn(N), np.random.randn(N), \"ko\")\n",
    "axs[1,0].set_title('Zero correlation',fontweight='bold')\n",
    "common_axs_setting(axs[1,0])\n",
    "\n",
    "# /20 to scale down\n",
    "x = np.cos(np.linspace(0, 2*np.pi, N)) + np.random.randn(N)/20\n",
    "y = np.sin(np.linspace(0, 2*np.pi, N)) + np.random.randn(N)/20\n",
    "axs[1,1].plot(x, y, \"ko\")\n",
    "axs[1,1].set_title('Zero correlation',fontweight='bold')\n",
    "common_axs_setting(axs[1,1])\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('Figure_04_01.png',dpi=300) # write out the fig to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that takes two vectors as input and provides two numbers as output: the Pearson correlation coefficient and the cosine similarity value. Write code that follows the formulas presented in this chapter; don’t simply call `np.corrcoef` and `spatial.distance`.cosine.</br>\n",
    "Check that the two output values are identical when the variables are already mean centered and different when the variables are not mean centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrAndCos(x, y):\n",
    "    \"\"\"\n",
    "    calcs cosine similarity and pearson corr.\n",
    "    \"\"\"\n",
    "    # cosine similarity\n",
    "    cos = np.dot(x, y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
    "    \n",
    "    # pearson corr.\n",
    "    xm = x - np.mean(x)\n",
    "    ym = y - np.mean(y)\n",
    "    num = np.dot(xm, ym)\n",
    "    den = (np.linalg.norm(xm)*np.linalg.norm(ym)) \n",
    "    cor = num/den\n",
    "    return cor, cos\n",
    "\n",
    "a = np.random.randn(3)\n",
    "b = np.random.randn(3)\n",
    "\n",
    "cor, cos = corrAndCos(a, b)\n",
    "np_ans = np.corrcoef(a, b)[0,1]\n",
    "print(f\"{cor = }\\n{cos = }\\n{np_ans = }\")\n",
    "assert round(cor, 3)==round(np_ans, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare r and c without mean-centering\n",
    "a = np.random.randn(3) + 10\n",
    "b = np.random.randn(3)\n",
    "\n",
    "c = a - np.mean(a)\n",
    "d = b - np.mean(b)\n",
    "\n",
    "print(f\"No mean center (should differ): {np.round(corrAndCos(a, b), 4)}\")\n",
    "print(f\"With mean center (should be same): {np.round(corrAndCos(c, d), 4)}\")\n",
    "print(\"Note that the pearson. corr for both cases are the same.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable containing the integers 0 through 3, and a second variable equaling the first variable plus some offset. \n",
    "\n",
    "You will then create a simulation in which you systematically vary that offset between −50 and +50 (that is, the first iteration of the simulation will have the second variable equal to [−50, −49, −48, −47]). \n",
    "\n",
    "In a for loop, compute the correlation and cosine similarity between the two variables and store these results. Then make a line plot showing how the correlation and cosine similarity are affected by the mean offset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(4, dtype=int) # essentially one of the vector for calc correlation\n",
    "offsets = np.arange(-50, 51)\n",
    "\n",
    "results = np.zeros((len(offsets), 2))\n",
    "\n",
    "for i in range(len(offsets)):\n",
    "    results[i,:] = corrAndCos(a, a+offsets[i]) \n",
    "    \n",
    "plt.figure(figsize=(8,4))\n",
    "h = plt.plot(offsets,results)\n",
    "h[0].set_color('k')\n",
    "h[0].set_marker('o')\n",
    "h[1].set_color([.7,.7,.7])\n",
    "h[1].set_marker('s')\n",
    "\n",
    "plt.xlabel('Mean offset')\n",
    "plt.ylabel('r or c')\n",
    "plt.legend(['Pearson','Cosine sim.'])\n",
    "# plt.savefig('Figure_04_02.png',dpi=300) # write out the fig to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EX3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several Python functions to compute the Pearson correlation coefficient. One of them is called pearsonr and is located in the stats module of the SciPy library. Open the source code for this file (hint: ??functionname) and make sure you understand how the Python implementation maps onto the formulas introduced in this chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "??pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you ever need to code your own functions when they already exist in Python? Part of the reason is that writing your own functions has huge educational value, because you see that (in this case) the correlation is a simple computation and not some incredibly sophisticated black-box algorithm that only a computer-science PhD could understand. But another reason is that built-in functions are sometimes slower because of myriad input checks, dealing with additional input options, converting data types, etc. This increases usability but at the expense of computation time.\n",
    "\n",
    "Your goal in this exercise is to determine whether your own bare-bones correlation function is faster than NumPy’s corrcoef function. Modify the function from Exercise 4-2 to compute only the correlation coefficient. Then, in a for loop over 1,000 iterations, generate two variables of 500 random numbers and compute the correlation between them. Time the for loop. Then repeat but using `np.corrcoef`. In my tests, the custom function was about 33% faster than np.corrcoef. In these toy examples, the differences are measured in milliseconds, but if you are running billions of correlations with large datasets, those milliseconds really add up! \n",
    "\n",
    "(Note that writing your own functions without input checks has the risk of input errors that would be caught by np.corrcoef.) (Also note that the speed advantage breaks down for larger vectors.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def rho(x,y):\n",
    "  xm = x-np.mean(x)\n",
    "  ym = y-np.mean(y)\n",
    "  n  = np.dot(xm,ym)\n",
    "  d  = np.linalg.norm(xm) * np.linalg.norm(ym)\n",
    "  return n/d\n",
    "\n",
    "\n",
    "it = 1000\n",
    "n = 500\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(it):\n",
    "  x = np.random.randn(n,2)\n",
    "  rho(x[:,0], x[:,1])\n",
    "t1 = time.time() - tic\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(it):\n",
    "  x = np.random.randn(n,2)\n",
    "  pearsonr(x[:,0], x[:,1])\n",
    "t2 = time.time() - tic\n",
    "\n",
    "# Note: time() returns seconds, so I multiply by 1000 for ms\n",
    "print(f'My function took {t1*1000:.2f} ms')\n",
    "print(f'   pearsonr took {t2*1000:.2f} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s build an edge detector. The kernel for an edge detector is very simple: [−1 +1]. The dot product of that kernel with a snippet of a time series signal with constant value (e.g., [10 10]) is 0. \n",
    "But that dot product is large when the signal has a steep change (e.g., [1 10] produces a dot product of 9). The signal we’ll work with is a plateau function. Graphs A and B in Figure 4-5 show the kernel and the signal. The first step in this exercise is to write code that creates these two time series.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\".\\imgs\\book\\plad_0405.png\" alt=\"\">\n",
    "</p>\n",
    "\n",
    "Next, write a for loop over the time points in the signal. At each time point, compute the dot product between the kernel and a segment of the time series data that has the same length as the kernel. You should produce a plot that looks like graph C in Figure 4-5. \n",
    "Notice that our edge detector returned 0 when the signal was flat, +1 when the signal jumped up, and −1 when the signal jumped down.\n",
    "\n",
    "Feel free to continue exploring this code. For example, does anything change if you pad the kernel with zeros ([0 −1 1 0])? What about if you flip the kernel to be [1 −1]? How about if the kernel is asymmetric ([−1 2])?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.array([-1, 1])\n",
    "\n",
    "signal = np.zeros(30)\n",
    "signal[10:20]=1\n",
    "\n",
    "feature_map = np.zeros(len(signal))\n",
    "\n",
    "for t in range(1, len(signal)-1):\n",
    "    feature_map[t] = np.dot(kernel, signal[t-1:t+1]) \n",
    "\n",
    "_, axs = plt.subplots(1,3, figsize=(15,3))\n",
    "axs[0].plot(kernel,\"ks-\")\n",
    "axs[0].set_title(\"kernel\")\n",
    "axs[0].set_xlim([-15,15])\n",
    "\n",
    "axs[1].plot(signal, 'ks-')\n",
    "axs[1].set_title(\"Time series signal\")\n",
    "\n",
    "axs[2].plot(signal, 'ks-', label=\"signal\", linewidth=3)\n",
    "markers, stemlines, _ = axs[2].stem(range(len(feature_map)), feature_map, \n",
    "                                    basefmt=\" \", linefmt = \"\", markerfmt=\"o\",\n",
    "                                    label=\"Edge detection\")\n",
    "plt.setp(stemlines, \"color\", [.7,.7,.7])\n",
    "plt.setp(markers, \"color\", [.7,.7,.7])\n",
    "\n",
    "\n",
    "axs[2].legend()\n",
    "# plt.savefig('Figure_04_04ac.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the kernel (a sorta-kinda Gaussian)\n",
    "kernel = np.array([0,.1,.3,.8,1,.8,.3,.1,0])\n",
    "kernel /= np.sum(kernel)\n",
    "\"\"\"\n",
    "Copilot\n",
    "Divide kernel by np.sum(kernel): This is done to normalize the kernel. In other words, \n",
    "the sum of all elements in the kernel will be 1 after this operation. \n",
    "\n",
    "This is a common practice when using kernels for convolution in signal processing, \n",
    "as it ensures that the overall energy of the \n",
    "signal is preserved after the convolution operation.\n",
    "\n",
    "Normalizing the kernel by its sum instead of its maximum value ensures that the overall energy or amplitude of the signal is \n",
    "preserved after the convolution operation.\n",
    "\n",
    "When you normalize by the sum, the total weight of the kernel is 1. This means that when you apply the kernel to the signal, \n",
    "the sum of the original values in the window covered by the kernel will be the same before and after the convolution. \n",
    "This is important in many applications where the absolute scale of the signal matters.\n",
    "\n",
    "If you normalize by the maximum value, the highest value in the kernel will be 1, but the sum of the kernel values may not be 1. \n",
    "This could result in the signal being amplified or attenuated after the convolution, depending on the shape of the kernel.\n",
    "\n",
    "In summary, normalizing by the sum is a common practice in signal processing to preserve the energy or amplitude of the signal during convolution operations. However, the appropriate normalization method can depend on the specific application and the properties of the kernel and signal. I hope this helps! Let me know if you have any other questions.\n",
    "\n",
    "Let us say we have a kernel of length 3: kernel = np.array([1, 2, 1]).\n",
    "If we normalize by the sum, we divide each element by the sum of all elements (which is 4 in this case):\n",
    "\n",
    "kernel = kernel / np.sum(kernel) \n",
    "       = [1/4, 2/4, 1/4] \n",
    "       = [0.25, 0.5, 0.25]\n",
    "\n",
    "Now, let us say we apply this kernel to a part of the signal that is [3, 4, 3]. The convolution operation (dot product in this case) would be:\n",
    "\n",
    "np.dot(kernel, signal) \n",
    "= 0.25*3 + 0.5*4 + 0.25*3 \n",
    "= 0.75 + 2 + 0.75 \n",
    "= 3.5\n",
    "\n",
    "Notice that the result (3.5) is close to the original center value of the signal (4). \n",
    "This is because the kernel is normalized by sum, which preserves the overall energy of the signal.\n",
    "\n",
    "On the other hand, if we normalize by the maximum value, the kernel becomes [1/2, 1, 1/2] = [0.5, 1, 0.5]. \n",
    "If we apply this kernel to the same part of the signal, the result would be:\n",
    "\n",
    "np.dot(kernel, signal) \n",
    "= 0.5*3 + 1*4 + 0.5*3 \n",
    "= 1.5 + 4 + 1.5 \n",
    "= 7\n",
    "\n",
    "This result (7) is larger than the original center value of the signal (4), which means the signal has been amplified. \n",
    "This is why normalizing by sum is often preferred in signal processing, as it preserves the overall energy of the signal.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# length param.\n",
    "Nkernel = len(kernel)\n",
    "halfKrn = Nkernel//2 # find midpoint\n",
    "\"\"\"\n",
    "Copilot\n",
    "finding the midpoint of the kernel. \n",
    "\n",
    "This is useful for the convolution operation, as the kernel is applied symmetrically \n",
    "around each point in the signal. By knowing the midpoint of the kernel, the coder can correctly\n",
    "align the kernel with the signal during the convolution operation. The // operator is used for\n",
    "integer division, which means that if the kernel length is odd, the midpoint will be rounded down. \n",
    "   \n",
    "For example, if Nkernel is 9, halfKrn will be 4. This means the\n",
    "kernel will be applied to the signal from 4 points before to 4 points after the current point. \n",
    "\"\"\"\n",
    "\n",
    "# and the signal\n",
    "Nsignal = 100\n",
    "timeseries = np.random.randn(Nsignal)\n",
    "\n",
    "# make a copy of the signal for filtering\n",
    "filtsig = timeseries.copy()\n",
    "\n",
    "# loop over the signal time points\n",
    "for t in range(halfKrn+1,Nsignal-halfKrn):\n",
    "  filtsig[t] = np.dot(kernel,timeseries[t-halfKrn-1:t+halfKrn])\n",
    "\n",
    "# plot them\n",
    "_,axs = plt.subplots(1,3,figsize=(25,4))\n",
    "axs[0].plot(kernel,'ks-')\n",
    "axs[0].set_title('Kernel')\n",
    "axs[0].set_xlim([-1,Nsignal])\n",
    "\n",
    "axs[1].plot(timeseries,'ks-')\n",
    "axs[1].set_title('Time series signal')\n",
    "\n",
    "axs[2].plot(timeseries,color='k',label='Original',linewidth=1)\n",
    "axs[2].plot(filtsig,'--',color=[.6,.6,.6],label='Smoothed',linewidth=2)\n",
    "axs[2].legend()\n",
    "\n",
    "# plt.savefig('Figure_04_06c.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex7\n",
    "Replace the 1 in the center of the kernel with −1 and mean center the kernel. Then rerun the filtering and plot code. What is the result? It actually accentuates the sharp features! In fact, this kernel is now a high-pass filter, meaning it dampens the smooth (low-frequency) features and highlights the rapidly changing (high-frequency) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the kernel (a sorta-kinda Gaussian)\n",
    "kernel = np.array([0,.1,.3,.8,-1,.8,.3,.1,0])\n",
    "kernel /= np.sum(kernel)\n",
    "kernel -= np.mean(kernel)\n",
    "\n",
    "_, axs = plt.subplots(1,3, figsize=(25,4))\n",
    "axs[0].plot(kernel, 's-')\n",
    "axs[0].set_title(\"Kernel\")\n",
    "axs[0].set_xlim([-1, Nsignal])\n",
    "\n",
    "axs[1].plot(timeseries, \"s-\")\n",
    "axs[1].set_title('Time series signal')\n",
    "\n",
    "filtsig2 = timeseries.copy()\n",
    "for t in range(halfKrn+1,Nsignal-halfKrn):\n",
    "  filtsig2[t] = np.dot(kernel,timeseries[t-halfKrn-1:t+halfKrn])\n",
    "\n",
    "plt.plot(timeseries,color='k',label='Original',linewidth=1)\n",
    "plt.plot(filtsig2,color=[.9,.2,.7],label='Sharpened',linewidth=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex8\n",
    "One way to determine an optimal k is to repeat the clustering multiple times (each time using randomly initialized cluster centroids) and assess whether the final clustering is the same or different. Without generating new data, rerun the k-means code several times using k = 3 to see whether the resulting clusters are similar (this is a qualitative assessment based on visual inspection). Do the final cluster assignments generally seem similar even though the centroids are randomly selected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create data\n",
    "nPerClust = 50\n",
    "\n",
    "# blur around centroid (std units)\n",
    "blur = 1\n",
    "\n",
    "# XY centroid locations\n",
    "A = [1, 1]\n",
    "B = [-3, 1]\n",
    "C = [3, 3]\n",
    "\n",
    "# generate data\n",
    "a = [A[0]+np.random.randn(nPerClust)*blur, A[1]+np.random.randn(nPerClust)*blur]\n",
    "b = [B[0]+np.random.randn(nPerClust)*blur, B[1]+np.random.randn(nPerClust)*blur]\n",
    "c = [C[0]+np.random.randn(nPerClust)*blur, C[1]+np.random.randn(nPerClust)*blur]\n",
    "\n",
    "# concatanate into a matrix\n",
    "data = np.transpose(np.concatenate((a,b,c),axis=1))\n",
    "\n",
    "# plot data\n",
    "plt.plot(data[:,0],data[:,1],'ko',markerfacecolor='w')\n",
    "plt.title('Raw (preclustered) data')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize random cluster centroids\n",
    "k = 3 # extract three clusters\n",
    "\n",
    "# random cluster centers (randomly sampled data points)\n",
    "ridx = np.random.choice(range(len(data)), k, replace=False)\n",
    "centroids = data[ridx,:]\n",
    "\n",
    "# setup the figure\n",
    "fig, axs = plt.subplots(2,2,figsize=(6,6))\n",
    "axs = axs.flatten()\n",
    "lineColors = [[0,0,0],[.4,.4,.4],[.8,.8,.8] ] #'rbm'\n",
    "\n",
    "# plot data with initial random cluster centroids\n",
    "axs[0].plot(data[:,0],data[:,1],'ko',markerfacecolor='w')\n",
    "axs[0].plot(centroids[:,0],centroids[:,1],'ko')\n",
    "axs[0].set_title('Iteration 0')\n",
    "axs[0].set_xticks([])\n",
    "axs[0].set_yticks([])\n",
    "\n",
    "# loop over iterations\n",
    "for iteri in range(3):\n",
    "    \n",
    "  # step 1: compute distances\n",
    "  dists = np.zeros((data.shape[0],k))\n",
    "  for ci in range(k):\n",
    "    dists[:,ci] = np.sum((data-centroids[ci,:])**2,axis=1)\n",
    "  \"\"\"\n",
    "  Copilot\n",
    "\n",
    "  we can vectorize this for loop with \n",
    "  dists = np.sum((data[:, np.newaxis] - centroids) **2, axis=2)\n",
    "  \"\"\"\n",
    "        \n",
    "  # step 2: assign to group based on minimum distance\n",
    "  groupidx = np.argmin(dists,axis=1)\n",
    "    \n",
    "  # step 3: recompute centers\n",
    "  for ki in range(k):\n",
    "    centroids[ki,:] = [np.mean(data[groupidx==ki,0]), \n",
    "                       np.mean(data[groupidx==ki,1]) ]\n",
    "  \n",
    "  # plot data points\n",
    "  for i in range(len(data)):\n",
    "    axs[iteri+1].plot([data[i,0], centroids[groupidx[i],0]],\n",
    "                      [data[i,1], centroids[groupidx[i],1]],\n",
    "                      color=lineColors[groupidx[i]])\n",
    "  axs[iteri+1].plot(centroids[:,0], centroids[:,1], 'ko')\n",
    "  axs[iteri+1].set_title(f'Iteration {iteri+1}')\n",
    "  axs[iteri+1].set_xticks([])\n",
    "  axs[iteri+1].set_yticks([])\n",
    "\n",
    "# plt.savefig('Figure_04_03.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex9\n",
    "Repeat the multiple clusterings using k = 2 and k = 4. What do you think of these results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize random cluster centroids\n",
    "k = 2 # extract three clusters\n",
    "\n",
    "# random cluster centers (randomly sampled data points)\n",
    "ridx = np.random.choice(range(len(data)), k, replace=False)\n",
    "centroids = data[ridx,:]\n",
    "\n",
    "# setup the figure\n",
    "fig, axs = plt.subplots(2,2,figsize=(6,6))\n",
    "axs = axs.flatten()\n",
    "lineColors = [[0,0,0],[.4,.4,.4],[.8,.8,.8] ] #'rbm'\n",
    "\n",
    "# plot data with initial random cluster centroids\n",
    "axs[0].plot(data[:,0],data[:,1],'ko',markerfacecolor='w')\n",
    "axs[0].plot(centroids[:,0],centroids[:,1],'ko')\n",
    "axs[0].set_title('Iteration 0')\n",
    "axs[0].set_xticks([])\n",
    "axs[0].set_yticks([])\n",
    "\n",
    "# loop over iterations\n",
    "for iteri in range(3):\n",
    "    \n",
    "  # step 1: compute distances\n",
    "  dists = np.zeros((data.shape[0],k))\n",
    "  for ci in range(k):\n",
    "    dists[:,ci] = np.sum((data-centroids[ci,:])**2,axis=1)\n",
    "  \"\"\"\n",
    "  Copilot\n",
    "\n",
    "  we can vectorize this for loop with \n",
    "  dists = np.sum((data[:, np.newaxis] - centroids) **2, axis=2)\n",
    "  \"\"\"\n",
    "        \n",
    "  # step 2: assign to group based on minimum distance\n",
    "  groupidx = np.argmin(dists,axis=1)\n",
    "    \n",
    "  # step 3: recompute centers\n",
    "  for ki in range(k):\n",
    "    centroids[ki,:] = [np.mean(data[groupidx==ki,0]), \n",
    "                       np.mean(data[groupidx==ki,1]) ]\n",
    "  \n",
    "  # plot data points\n",
    "  for i in range(len(data)):\n",
    "    axs[iteri+1].plot([data[i,0], centroids[groupidx[i],0]],\n",
    "                      [data[i,1], centroids[groupidx[i],1]],\n",
    "                      color=lineColors[groupidx[i]])\n",
    "  axs[iteri+1].plot(centroids[:,0], centroids[:,1], 'ko')\n",
    "  axs[iteri+1].set_title(f'Iteration {iteri+1}')\n",
    "  axs[iteri+1].set_xticks([])\n",
    "  axs[iteri+1].set_yticks([])\n",
    "\n",
    "# plt.savefig('Figure_04_03.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize random cluster centroids\n",
    "k = 4 # extract three clusters\n",
    "\n",
    "# random cluster centers (randomly sampled data points)\n",
    "ridx = np.random.choice(range(len(data)), k, replace=False)\n",
    "centroids = data[ridx,:]\n",
    "\n",
    "# setup the figure\n",
    "fig, axs = plt.subplots(2,2,figsize=(6,6))\n",
    "axs = axs.flatten()\n",
    "lineColors = [[0,0,0],[.4,.4,.4],[.8,.8,.8],[.6,.6,.6]] #'rbm'\n",
    "\n",
    "# plot data with initial random cluster centroids\n",
    "axs[0].plot(data[:,0],data[:,1],'ko',markerfacecolor='w')\n",
    "axs[0].plot(centroids[:,0],centroids[:,1],'ko')\n",
    "axs[0].set_title('Iteration 0')\n",
    "axs[0].set_xticks([])\n",
    "axs[0].set_yticks([])\n",
    "\n",
    "# loop over iterations\n",
    "for iteri in range(3):\n",
    "    \n",
    "  # step 1: compute distances\n",
    "  dists = np.zeros((data.shape[0],k))\n",
    "  for ci in range(k):\n",
    "    dists[:,ci] = np.sum((data-centroids[ci,:])**2,axis=1)\n",
    "  \"\"\"\n",
    "  Copilot\n",
    "\n",
    "  we can vectorize this for loop with \n",
    "  dists = np.sum((data[:, np.newaxis] - centroids) **2, axis=2)\n",
    "  \"\"\"\n",
    "        \n",
    "  # step 2: assign to group based on minimum distance\n",
    "  groupidx = np.argmin(dists,axis=1)\n",
    "    \n",
    "  # step 3: recompute centers\n",
    "  for ki in range(k):\n",
    "    centroids[ki,:] = [np.mean(data[groupidx==ki,0]), \n",
    "                       np.mean(data[groupidx==ki,1]) ]\n",
    "  \n",
    "  # plot data points\n",
    "  for i in range(len(data)):\n",
    "    axs[iteri+1].plot([data[i,0], centroids[groupidx[i],0]],\n",
    "                      [data[i,1], centroids[groupidx[i],1]],\n",
    "                      color=lineColors[groupidx[i]])\n",
    "  axs[iteri+1].plot(centroids[:,0], centroids[:,1], 'ko')\n",
    "  axs[iteri+1].set_title(f'Iteration {iteri+1}')\n",
    "  axs[iteri+1].set_xticks([])\n",
    "  axs[iteri+1].set_yticks([])\n",
    "\n",
    "# plt.savefig('Figure_04_03.png',dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
