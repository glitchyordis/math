{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NOTE: these lines define global figure properties used for publication.\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg') # print figures in svg format\n",
    "plt.rcParams.update({'font.size':14}) # set global font size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "\n",
    "# correlated random var\n",
    "x = np.linspace(0,10,N) + np.random.rand(N)\n",
    "y = x + np.random.randn(N)\n",
    "\n",
    "# setup figure\n",
    "_, axs = plt.subplots(2, 2, figsize=(6,6))\n",
    "\n",
    "def common_axs_setting(axs):\n",
    "    axs.set_xlabel(\"Variable x\")\n",
    "    axs.set_ylabel(\"Variable y\")\n",
    "    axs.set_xticks([])\n",
    "    axs.set_yticks([])\n",
    "    axs.axis(\"square\")\n",
    "\n",
    "axs[0,0].plot(x, y, \"ko\")\n",
    "axs[0,0].set_title(\"Positive correlation\", fontweight=\"bold\")\n",
    "common_axs_setting(axs[0,0])\n",
    "\n",
    "axs[0,1].plot(x, -y, \"ko\")\n",
    "axs[0,1].set_title(\"negative correlation\", fontweight=\"bold\")\n",
    "common_axs_setting(axs[0,1])\n",
    "\n",
    "axs[1,0].plot(np.random.randn(N), np.random.randn(N), \"ko\")\n",
    "axs[1,0].set_title('Zero correlation',fontweight='bold')\n",
    "common_axs_setting(axs[1,0])\n",
    "\n",
    "# /20 to scale down\n",
    "x = np.cos(np.linspace(0, 2*np.pi, N)) + np.random.randn(N)/20\n",
    "y = np.sin(np.linspace(0, 2*np.pi, N)) + np.random.randn(N)/20\n",
    "axs[1,1].plot(x, y, \"ko\")\n",
    "axs[1,1].set_title('Zero correlation',fontweight='bold')\n",
    "common_axs_setting(axs[1,1])\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('Figure_04_01.png',dpi=300) # write out the fig to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that takes two vectors as input and provides two numbers as output: the Pearson correlation coefficient and the cosine similarity value. Write code that follows the formulas presented in this chapter; don’t simply call `np.corrcoef` and `spatial.distance`.cosine.</br>\n",
    "Check that the two output values are identical when the variables are already mean centered and different when the variables are not mean centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrAndCos(x, y):\n",
    "    \"\"\"\n",
    "    calcs cosine similarity and pearson corr.\n",
    "    \"\"\"\n",
    "    # cosine similarity\n",
    "    cos = np.dot(x, y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
    "    \n",
    "    # pearson corr.\n",
    "    xm = x - np.mean(x)\n",
    "    ym = y - np.mean(y)\n",
    "    num = np.dot(xm, ym)\n",
    "    den = (np.linalg.norm(xm)*np.linalg.norm(ym)) \n",
    "    cor = num/den\n",
    "    return cor, cos\n",
    "\n",
    "a = np.random.randn(3)\n",
    "b = np.random.randn(3)\n",
    "\n",
    "cor, cos = corrAndCos(a, b)\n",
    "np_ans = np.corrcoef(a, b)[0,1]\n",
    "print(f\"{cor = }\\n{cos = }\\n{np_ans = }\")\n",
    "assert round(cor, 3)==round(np_ans, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare r and c without mean-centering\n",
    "a = np.random.randn(3) + 10\n",
    "b = np.random.randn(3)\n",
    "\n",
    "c = a - np.mean(a)\n",
    "d = b - np.mean(b)\n",
    "\n",
    "print(f\"No mean center (should differ): {np.round(corrAndCos(a, b), 4)}\")\n",
    "print(f\"With mean center (should be same): {np.round(corrAndCos(c, d), 4)}\")\n",
    "print(\"Note that the pearson. corr for both cases are the same.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable containing the integers 0 through 3, and a second variable equaling the first variable plus some offset. \n",
    "\n",
    "You will then create a simulation in which you systematically vary that offset between −50 and +50 (that is, the first iteration of the simulation will have the second variable equal to [−50, −49, −48, −47]). \n",
    "\n",
    "In a for loop, compute the correlation and cosine similarity between the two variables and store these results. Then make a line plot showing how the correlation and cosine similarity are affected by the mean offset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(4, dtype=int) # essentially one of the vector for calc correlation\n",
    "offsets = np.arange(-50, 51)\n",
    "\n",
    "results = np.zeros((len(offsets), 2))\n",
    "\n",
    "for i in range(len(offsets)):\n",
    "    results[i,:] = corrAndCos(a, a+offsets[i]) \n",
    "    \n",
    "plt.figure(figsize=(8,4))\n",
    "h = plt.plot(offsets,results)\n",
    "h[0].set_color('k')\n",
    "h[0].set_marker('o')\n",
    "h[1].set_color([.7,.7,.7])\n",
    "h[1].set_marker('s')\n",
    "\n",
    "plt.xlabel('Mean offset')\n",
    "plt.ylabel('r or c')\n",
    "plt.legend(['Pearson','Cosine sim.'])\n",
    "# plt.savefig('Figure_04_02.png',dpi=300) # write out the fig to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EX3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several Python functions to compute the Pearson correlation coefficient. One of them is called pearsonr and is located in the stats module of the SciPy library. Open the source code for this file (hint: ??functionname) and make sure you understand how the Python implementation maps onto the formulas introduced in this chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "??pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you ever need to code your own functions when they already exist in Python? Part of the reason is that writing your own functions has huge educational value, because you see that (in this case) the correlation is a simple computation and not some incredibly sophisticated black-box algorithm that only a computer-science PhD could understand. But another reason is that built-in functions are sometimes slower because of myriad input checks, dealing with additional input options, converting data types, etc. This increases usability but at the expense of computation time.\n",
    "\n",
    "Your goal in this exercise is to determine whether your own bare-bones correlation function is faster than NumPy’s corrcoef function. Modify the function from Exercise 4-2 to compute only the correlation coefficient. Then, in a for loop over 1,000 iterations, generate two variables of 500 random numbers and compute the correlation between them. Time the for loop. Then repeat but using `np.corrcoef`. In my tests, the custom function was about 33% faster than np.corrcoef. In these toy examples, the differences are measured in milliseconds, but if you are running billions of correlations with large datasets, those milliseconds really add up! \n",
    "\n",
    "(Note that writing your own functions without input checks has the risk of input errors that would be caught by np.corrcoef.) (Also note that the speed advantage breaks down for larger vectors.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def rho(x,y):\n",
    "  xm = x-np.mean(x)\n",
    "  ym = y-np.mean(y)\n",
    "  n  = np.dot(xm,ym)\n",
    "  d  = np.linalg.norm(xm) * np.linalg.norm(ym)\n",
    "  return n/d\n",
    "\n",
    "\n",
    "it = 1000\n",
    "n = 500\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(it):\n",
    "  x = np.random.randn(n,2)\n",
    "  rho(x[:,0], x[:,1])\n",
    "t1 = time.time() - tic\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(it):\n",
    "  x = np.random.randn(n,2)\n",
    "  pearsonr(x[:,0], x[:,1])\n",
    "t2 = time.time() - tic\n",
    "\n",
    "# Note: time() returns seconds, so I multiply by 1000 for ms\n",
    "print(f'My function took {t1*1000:.2f} ms')\n",
    "print(f'   pearsonr took {t2*1000:.2f} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s build an edge detector. The kernel for an edge detector is very simple: [−1 +1]. The dot product of that kernel with a snippet of a time series signal with constant value (e.g., [10 10]) is 0. But that dot product is large when the signal has a steep change (e.g., [1 10] would produce a dot product of 9). The signal we’ll work with is a plateau function. Graphs A and B in Figure 4-5 show the kernel and the signal. The first step in this exercise is to write code that creates these two time series.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\".\\downloads\\imgs\\plad_0405.png\" alt=\"\">\n",
    "</p>\n",
    "\n",
    "Next, write a for loop over the time points in the signal. At each time point, compute the dot product between the kernel and a segment of the time series data that has the same length as the kernel. You should produce a plot that looks like graph C in Figure 4-5. (Focus more on the result than on the aesthetics.) Notice that our edge detector returned 0 when the signal was flat, +1 when the signal jumped up, and −1 when the signal jumped down.\n",
    "\n",
    "Feel free to continue exploring this code. For example, does anything change if you pad the kernel with zeros ([0 −1 1 0])? What about if you flip the kernel to be [1 −1]? How about if the kernel is asymmetric ([−1 2])?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randn(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3 # number of random cluster centroids\n",
    "\n",
    "data = np.random.rand(150,2)\n",
    "ridx = np.random.choice(range(len(data)), k, replace=False)\n",
    "centroids = data[ridx, :]\n",
    "\n",
    "print(f\"{ridx = }\\ncentroids:\\n{centroids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(range(len(data)), k, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = np.zeros((data.shape[0], k))\n",
    "for ci in range(k):\n",
    "    dists[:, ci] = np.sum((data-centroids[ci,:])**2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupidx = np.argmin(dists, axis=1)\n",
    "print(f\"{groupidx = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ki in range(k):\n",
    "    centroids[ki, :] = [np.mean(data[groupidx==ki, 0]),\n",
    "                        np.mean(data[groupidx==ki, 1])]\n",
    "\n",
    "print(f\"centroids\\n{centroids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((data[:, np.newaxis] - centroids) **2, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([np.mean(data[groupidx==ki], axis=0) for ki in range(k)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
